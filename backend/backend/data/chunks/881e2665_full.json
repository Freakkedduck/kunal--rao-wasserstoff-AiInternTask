{
  "doc_id": "881e2665",
  "filename": "Kunal_Rao_102119021_Eval_final.pdf",
  "content": [
    {
      "page": 1,
      "text": "Final Report: Project Semester\non\nEnhancing Quality Assurance Through Real-Time Log Analytic\nDashboard & AI-Powered Test Case Generation Agent\nSubmitted by\nKunal Rao\n10211021\nB.E Electrical and Computer Engineering\nUnder the Guidance of\nHost Mentor\nMr. Aashish Paliwal\nModule Lead\nLogic Fruit Technologies\nFaculty Supervisor\nDr. Alok Kumar Shukla\nAssistant Professor\nDEIE, TIET, Patiala\n2025\nDepartment of Electrical and Instrumentation Engineering\nThapar Institute of Engineering and Technology, Patiala\n(Declared as Deemed-to-be-University u/s 3 of the UGC Act., 1956)\nPost Bag No. 32, Patiala \u2013 147004\nPunjab (India)"
    },
    {
      "page": 2,
      "text": "To Whomsoever it may concern\nThis is to certify that Kunal Rao, a student of Thapar Institute of Engineering and\nTechnology, Patiala, has undertaken a project titled \u201cEnhancing Quality Assurance\nThrough Real-Time Log Analytic Dashboard & AI-Powered Test Case Generation\nAgent\u201d during his industrial training program at Logic Fruit Technologies Pvt. Ltd.\nThe organization has no objection to the submission of this report to TIET for the purpose of\nfinal presentation and viva, as part of the requirements for the award of the B.E. degree in\nElectrical and Computer Engineering.\nThe industrial training program commenced on 3rd March and is currently ongoing.\nName of Industrial Mentor: Mr. Aashish Paliwal\nName of student: Kunal Rao\nSignature of Industrial Mentor:\nSignature of Student:\nDate: 2nd June, 2025\nPlace: Gurugram, Haryana"
    },
    {
      "page": 3,
      "text": "i\nDeclaration\nI hereby declare that the midway report titled, \u201cEnhancing Quality Assurance Through\nReal-Time Log Analytics Dashboard & AI-Powered Test Case Generation Agent\u201d,\nsubmitted as a partial requirement for the Project Semester (ULC891) course towards the\nBachelor of Engineering degree in Electrical and Computer Engineering at Thapar Institute of\nEngineering and Technology, Patiala, is an accurate representation of the work I undertook.\nThis work is carried out under the guidance and supervision of Mr. Aashish Paliwal, Module\nLead at Logic Fruit Technologies (Host Mentor), and Dr. Alok Kumar Shukla, Assistant\nProfessor at the Department of Electrical and Instrumentation Engineering, TIET, Patiala,\nIndia.\nPlace: Patiala\nKunal Rao\nDate: 23rd April, 2025\n102119021\nIt is certified that the above statement made by the student is correct to the best of my\nknowledge and belief.\nHost Mentor\nMr. Aashish Paliwal\nModule Lead\nLogic Fruit Technologies, Gurugram\nFaculty Supervisor\nDr. Alok Kumar Shukla\nAssistant Professor\nDEIE, TIET, Patiala"
    },
    {
      "page": 4,
      "text": "ii\nAcknowledgement\nI wish to sincerely thank my host mentor Mr. Aashish Paliwal for his valuable guidance and\ncontinuous support during this project semester. I also express my gratitude to Dr. Alok\nKumar Shukla, my faculty supervisor, for his consistent advice and helpful insights. My\nappreciation also extends to the team at Logic Fruit Technologies for their assistance,\ncooperation, and the resources provided, all of which significantly contributed to the progress\nof my project.\nKunal Rao\n102119021"
    },
    {
      "page": 5,
      "text": "iii\nAbstract\nThis report outlines the progress of the project titled \u201cEnhancing Quality Assurance\nThrough Real-Time Log Analytics Dashboard & AI-Powered Test Case Generation\nAgent\u201d undertaken at Logic Fruit Technologies. The project aims to enhance quality\nassurance by implementing an automated monitoring system via a real-time log analytics\ndashboard and developing an AI-powered test case generation agent using LangChain and the\nLlama3.1 8B model.\nAt this stage, the real-time log analytics dashboard has been successfully developed,\nintegrated with the generic test automation framework. The AI-based test case generation\nagent, named as \u201cTCG Agent\u201d has also been developed and is currently undergoing\nvalidation with various Software Requirements Documents (SRDs), only deployment remains.\nThe next steps involve completing the deployment of the test case generation agent,\nconducting extensive testing, debugging, performance optimization, and preparing for final\ndelivery of the complete system."
    },
    {
      "page": 6,
      "text": "iv\nTable of Contents\nDeclaration\ni\nAcknowledgement\nii\nAbstract\niii\nList of Figures\nvii\nList of Abbreviations\nviii\n1.\nIntroduction\n1\n1.1\nIntroduction\n1\n1.1.1\nCompany Profile\n1\n1.1.2\nRole of Quality Assurance in Software Development\n1-2\n1.2\nReal-Time Log Analytics Dashboard\n3-4\n1.3\nAI-Powered Test Case Generation Agent\n4-5\n2.\nProject Objective and Goal\n6\n2.1\nGoals\n6\n2.2\nObjectives\n6\n2.2.1\nReal-Time Log Analytics Dashboard\n6\n2.2.2\nAI-Powered Test Case Generation Agent\n6-7\n3.\nLiterature Review\n8\n3.1\nReal-Time Log Analytics\n8\n3.2\nData Visualization and Dashboard design\n8-9\n3.3\nAutomated Test Case Generation\n9-10\n3.4\nFrameworks: LangChain and Llama Model\n10-11"
    },
    {
      "page": 7,
      "text": "v\n3.5\nSummary of Research Gap\n11\n4.\nTechnical Challenges and Solutions\n4.1\nChallenge: Long and Complex SRDs\n12\n4.2\nChallenge: Prompt Injection and Model Hallucination\n12-13\n4.3\nChallenge: Dashboard Performance Lag Under High-Frequency Input\n13\n4.4\nChallenge: Format Preservation in Exported Test Case Documents\n13-14\n5.\nMethodology\n15\n5.1\nReal-Time Log Analytics Dashboard\n15\n5.1.1\nData Collection and Aggregation\n15\n5.1.2\nData Processing and Parsing\n15\n5.1.3\nReal-Time Analysis and Monitoring\n15\n5.1.4\nDashboard Design and Visualization\n15\n5.2\nAI-Powered Test Case Generation Agent\n16\n5.2.1\nRequirement Analysis and Contextual Understanding\n16-17\n5.2.2\nData Preparation and Model Fine-tuning\n17\n5.2.2.1\nSRD to Test Case mapping\n17\n5.2.2.2\nConversion to Alpaca format\n17\n5.2.2.3\nDataset evaluation and refinement\n18\n5.2.3\nTest Scenario Generation\n18\n5.2.4\nIntegration into QA Workflow\n18-19\n6.\nConclusion and Future Steps\n20\n6.1\nConclusion\n20\n6.2\nFuture Steps\n20-21\n7.\nReferences\n22-23"
    },
    {
      "page": 8,
      "text": "vi\nList of Figures\n1.2.1\nReal-time QA workflow integration\n3\n1.2.2\nReal-time Log analytics Dashboard\n4\n1.2.3\nTest cycle analytics tab\n4\n4.1\nFlowchart: Log analytics dashboard\n12\n4.2\nFlowchart: Test Case Generation Agent\n14"
    },
    {
      "page": 9,
      "text": "vii\nList of Abbreviations\nQA\nQuality Assurance\nAI\nArtificial Intelligence\nUI\nUser Interface\nCI/CD\nContinuous Integration / Continuous Deployment\nJSON\nJavaScript object Notation\nCSV\nComma Seperated Values\nXML\nExtensible Markup Language\nAPI\nApplication Programming Interface\nML\nMachine Learing\nNLP\nNatural Language Processing\nLLM\nLarge Language Model\nSRS\nSofware Requirement Specification\nGPU\nGraphical Processing Unit\nKPI\nKey Performance Indicator"
    },
    {
      "page": 10,
      "text": "1\nChapter 1\nIntroduction\n1.1 Introduction\n1.1.1 Company Profile\nLogic Fruit Technologies Pvt. Ltd. is an engineering firm that focuses on developing practical\nsolutions in areas like embedded systems, digital signal processing, and automated testing.\nThe company works on specialized projects involving FPGA design, high-speed circuit\nboards, and custom protocols, primarily serving industries such as aerospace, defence, and\ntelecom.\nOver the years, Logic Fruit Technologies has built a reputation for delivering technically\nsound, reliable products to clients in both Indian and international markets. The company\nemphasizes hands-on problem-solving and encourages a strong learning environment, where\nengineers and interns are given the opportunity to engage with complex, real-world\nchallenges.\nAs part of my industrial training, I was placed in the R&D department, where I contributed to\nprojects aimed at improving software quality assurance. My work involved developing\nsystems for log analysis and test case generation. The support from mentors and access to\ntechnical resources at the company played a key role in helping me complete this project\nsuccessfully.\n1.1.2 Role of Quality Assurance in Software Development\nSoftware development is changing and there is a need for more advanced Quality Assurance\n(QA) to ensure complex systems are reliable, performant, and stable. Legacy QA approaches\nsimply can\u2019t keep up with the fast development cycles and technical complexity of today\u2019s\napplications, no matter how fundamental they are. This has precipitated an explosion of\n\u2018next-gen scrappy QA techniques\u2019 that rely on automation/artificial intelligence/real-time\nanalytics to deliver effective solutions to these pressing challenges."
    },
    {
      "page": 11,
      "text": "2\nA major improvement was the use of real-time log analytics dashboards. These dashboards\nprovide teams instant, actionable insights on software performance and stability by\nvisualizing testing data, trends and outcomes in real-time. Through the automation of test\nexecution, combined with reporting dashboards, teams are able to expedite testing, pinpoint\nissues faster and make decisions based on empirical data without blocking development.\nSuch strategy fosters cooperation and maintains good quality during the whole cycle of\nsoftware development [1].\nThe scope of the software testing is already being redefined by AI and machine learning, too.\nAI Based Tools will be able to automatically create and sustain the test cases by examining\nthe requirements, user stories and history. This not only decreases the amount of manual\nlabor needed, but also provides a more thorough test coverage, even for edge cases that the\nhumans might forget. Predictive analytics then also h elp teams to move the needle on\nidentifying potential hotspots that can be tested and resolved early[2][4].\nThe intelligent Test Case Generation Agent makes a basic test framework for manual testers.\nBy leveraging llama3.1-8B fine-tuned on custom dataset , this agent can continuously adapt\ntest cases in real time to align with evolving application requirements. Such automation\naccelerates the creation and execution of tests, minimizes maintenance efforts through self-\nhealing scripts, and delivers deeper test coverage than manual methods. This results in faster\nrelease cycles, reduced costs, and improved overall product quality [3][4].\nAdditionally, incorporating these technologies into continuous integration and delivery\n(CI/CD) pipelines facilitates ongoing testing. Automated and AI-based testing tools can\ninitiate tests with each code modification, guaranteeing that issues are identified promptly\nand application stability is maintained throughout the software development process [1][2].\nIn Conclusion, the combination of real-time analytics and smart automation enables QA\nteams to concentrate on strategic activities, like exploratory testing and enhancing user\nexperiences, while technology efficiently handles routine and repetitive tasks. In conclusion,\nthe merging of real-time log analytics and AI-powered test case generation is revolutionizing\nsoftware QA"
    },
    {
      "page": 12,
      "text": "3\n1.2 Real-Time Log Analytics Dashboard\nThe fundamental to maintaining a robust system performance is log analytics. As systems\ngrow in complexity, the sheer volume and diversity of log data generated from applications,\ninfrastructure, and network devices can quickly overwhelm traditional manual inspection\nmethods, making them inefficient and susceptible to oversight and even negligence. The\nReal-Time Log Analytics Dashboard that is developed, directly addresses these challenges by\nautomating the aggregation, analysis, and visualization of log data, thereby transforming raw\nlogs into actionable insight.\nThe main aim of real-time log analytics dashboards is to centralize and present log data in an\nintuitive, easy-to-read format. By capturing logs, metrics, and traces into a single unified\nview, teams can efficiently monitor critical metrics from one console. This centralization is\nparticularly valuable for distributed systems and cloud-native architectures, where logs are\ngenerated from multiple sources and environments. The dashboard enables users to detect\nanomalies, observe trends, and make proactive, data-driven decisions, greatly improving\noperational efficiency and reducing response times to incidents.\nProactive monitoring is another core benefit of real-time log analytics. The dashboard\ncontinuously scans for application crashes, unexpected shutdowns, security and other\noperational events, providing immediate alerts when issues arise.\nFigure 1.2.1: Real-Time QA workflow Integration"
    },
    {
      "page": 13,
      "text": "4\nMoreover, real-time dashboards empower manual testers to conduct detailed testing. This\ncapability is essential for understanding the root cause of incidents, identifying long-term\ntrends, and ensuring compliance with security and operational standards. Advanced\ndashboards often incorporate artificial intelligence and machine learning to enhance anomaly\ndetection,\npattern\nrecognition,\nand\npredictive\nanalytics,\nfurther\nstrengthening\nan\norganization\u2019s ability to anticipate and mitigate risks.\nFigure 2.2.2: Real-Time Log Analytics Dashboard\nFigure 3.2.3: Test Cycle Analytics Tab"
    },
    {
      "page": 14,
      "text": "5\nIn summary, the Real-Time Log Analytics Dashboard transforms the way organizations\napproach system monitoring and incident response. By automating log data collection,\nanalysis, and visualization, it delivers immediate insights, supports proactive operations, and\nenhances both performance and security across complex software infrastructures.\n1.3 AI-Powered Test Case Generation Agent\nAutomated test case generation marks a transformative improvement in the way Quality\nAssurance (QA) processes are handled. In conventional workflows, test cases are manually\nwritten\u2014a time-consuming task that demands significant human effort and is prone to\ninconsistency and oversight. These limitations often result in inadequate test coverage,\nallowing bugs to go undetected and increasing the time and cost required for later fixes. In\ncontrast, automated test generation ensures more consistent and thorough validation of\nsoftware behavior across a wide range of scenarios.\nIn this project, I have successfully developed a Test Case Generation Agent that utilizes the\nLangChain framework in combination with the LLaMA 3.1 8B language model, which is\nfine-tuned on custom dataset. LangChain enables the orchestration of multiple AI-driven\ntasks, maintaining context and coherence throughout the generation pipeline. Paired with\nLLaMA 3.1 8B\u2019s strong natural language capabilities, this setup allows the agent to generate\nwell-structured, detailed, and context-aware test scenarios based on software requirements.\nFigure 1.3.1: Test case generation agent user interface"
    },
    {
      "page": 15,
      "text": "6\nWith the development phase completed, the agent is now undergoing validation using a\nvariety of Software Requirement Documents (SRDs) to ensure its effectiveness and\nadaptability. Once testing is finalized, it will be deployed on the company\u2019s server or on a\nstandalone system configured for that purpose. This approach not only reduces manual\nintervention but also minimizes human error, ensures better test coverage, and allows the\ntesting process to evolve alongside software changes. Overall, this agent stands as a\nsignificant leap forward in automated software testing, improving both the speed and\nreliability of QA workflows."
    },
    {
      "page": 16,
      "text": "7\nChapter 2\nProject Objective and Goal\nThis project, titled \"Enhancing Quality Assurance Through Real-Time Log Analytics\nDashboard & Test Case Generation Agent,\" aims to strengthen software Quality Assurance\n(QA)\nby\nintroducing\ninnovative\nsolutions.\nTo\nensure\nclarity,\neffectiveness,\nand\ncomprehensive coverage, the objectives of this project have been structured into two main\nmodules: the Real-Time Log Analytics Dashboard and the Intelligent Test Case Generation\nAgent.\n2.1 Goals\nThe primary\ngoal\nof this\nproject\nis to significantly\nimprove software\nreliability,\nmaintainability, and quality assurance efficiency at Logic Fruit Technologies by developing:\n\uf06c\nA robust, scalable, and intuitive Real-Time Log Analytics Dashboard capable of\naggregating and visualizing diverse log data.\n\uf06c\nA sophisticated Test Case Generation Agent leveraging advanced AI language\nmodeling frameworks\u2014LangChain combined with the Llama3.1 8B model, fine-\ntuned on custom dataset\u2014to automate and optimize test scenario creation.\nThese goals aim to substantially reduce manual intervention, minimize errors, and enhance\nsystem transparency, thereby promoting proactive management and preventive measures in\nsoftware quality assurance.\n2.2 Objectives\nThe detailed objectives supporting the realization of these overarching goals are outlined as\nfollows:\n2.2.1 Real-Time Log Analytics Dashboard\nI.\nCreating a user-centric, interactive dashboard designed to provide intuitive visual\nrepresentations of aggregated log data. .\nII.\nImplementing dynamic monitoring capabilities that facilitate the real-time tracking of\nsoftware systems."
    },
    {
      "page": 17,
      "text": "8\nIII. Ensuring the log analytics dashboard is capable of processing large data volumes swiftly\nwithout any significant degradation in performance.\n2.2.2 Test Case Generation Agent\nI.\nMapping SRDs and ATPs to create a dataset for fine-tuning Llama3.1-8B\nII.\nDeveloping a fully automated Test Case Generation Agent utilizing LangChain\nframework with the Llama3.1 8B advanced language model.\nIII. Creating UI for the test case agent\nIV. Ensuring the generated test cases are comprehensive, covering all relevant functional\nrequirements and edge cases effectively.\nThrough these clearly defined objectives, the project addresses significant challenges in\nmodern software QA processes. Each objective is systematically aligned with the overall goal\nof enhancing software reliability, efficiency, and responsiveness, ultimately leading to\nimproved productivity, reduced downtime, and superior end-user satisfaction."
    },
    {
      "page": 18,
      "text": "9\nChapter 3\nLiterature Review\nThis chapter provides a comprehensive analysis of existing literature that informs the design\nand development of the two major components of this project: the Real-Time Log Analytics\nDashboard and the AI-powered Test Case Generation Agent. These components, although\ndistinct in function, are closely interconnected in the larger context of modernizing software\nquality assurance practices. The review explores four core themes: the growing role of real-\ntime log analytics in systems monitoring, the critical role of visualization in interpreting\ncomplex data, the evolution of test case automation, and the emergence of advanced\nframeworks and models such as LangChain and LLaMA 3.1 8B in intelligent software\nengineering.\n3.1 Real-Time Log Analytics\nThe increasing complexity of digital infrastructure has led to an explosion in the volume,\nvelocity, and variety of system-generated logs. Logs capture vital system activities such as\nerrors,\ntransactions,\nrequests,\nresource\nusage,\nand\nperformance\nmetrics,\nwhich\nare\nindispensable for operational insights. However, static log inspection is inefficient in fast-\npaced environments. Real-time log analytics addresses this challenge by enabling continuous\nmonitoring and real-time anomaly detection.\nLiu et al. (2019) report that the implementation of real-time log analysis significantly reduces\ndowntime by enabling teams to detect and resolve issues before they cause critical failures.\nThis proactive capability transforms reactive IT support into intelligent, preventive\nmonitoring. For mission-critical industries such as finance, healthcare, and aviation, such\nreal-time capabilities can mean the difference between service continuity and catastrophic\ndisruption.\nMoreover, Vu et al. (2018) emphasize the importance of aggregating logs from disparate\nsources into a centralized, normalized stream for better visibility. Their study demonstrates\nhow visual dashboards allow IT administrators to track key performance indicators (KPIs),\nview real-time system health, and set alert thresholds for anomalies. These dashboards are\nfoundational to modern observability platforms and DevOps pipelines."
    },
    {
      "page": 19,
      "text": "10\nIn this project, real-time log analytics is realized through a custom-built dashboard hosted on\nStreamlit. This dashboard receives logs from automated testing activities, aggregates them,\nand presents visual summaries to support debugging, reporting, and decision-making during\nthe test lifecycle.\n3.2 Data Visualization and Dashboard Design\nData visualization is the science and art of transforming abstract information into concrete\nvisual representations that enhance understanding. In software quality assurance, where\nthousands of data points are generated per test cycle, visualization helps teams make quick\nand informed decisions. A well-designed dashboard not only presents data but also tells a\nstory that is meaningful to its users.\nFew (2013) posits that effective dashboard design requires clarity, focus, and accessibility.\nHe notes that dashboards must reduce cognitive friction by organizing data according to\npriority and context. Data overload can paralyze decision-making; hence, good design\npractices such as grouping related metrics, using color judiciously, and eliminating\nunnecessary visual noise are essential.\nKnaflic (2015) builds on this by promoting visual storytelling as a method of enhancing\nengagement and comprehension. Her work emphasizes the role of layout, annotation, and\ninteractivity in guiding users toward insights. These principles directly influenced the design\nof the project dashboard, which features time-series visualizations, event filters, and test case\nsummaries.\nIncorporating best practices from these studies, the dashboard developed in this project\nbridges the gap between raw log data and meaningful QA metrics. It empowers users to not\nonly monitor but also interpret the outcomes of automated test runs in real time.\n3.3 Automated Test Case Generation\nManual test case development, while precise, is time-consuming and often limited in scope.\nAs software evolves rapidly through agile development cycles, the need for automation in test\ngeneration has become critical. Automated test case generation reduces manual overhead,\nimproves coverage, and allows for continuous testing."
    },
    {
      "page": 20,
      "text": "11\nAnand et al. (2013) argue that automation supports scalability and consistency in software\ntesting, particularly in regression suites. Their work highlights that automated test generation\nfrom requirements improves traceability and reduces human error. However, conventional\nautomation tools still rely heavily on structured inputs and predefined rules.\nWith the advent of AI and machine learning, especially natural language processing (NLP), it\nhas become possible to derive test cases directly from unstructured documents such as SRDs\n(Software Requirement Documents). Sanguesa et al. (2021) explore how transformer-based\nmodels, trained on curated datasets, can understand requirements and produce semantically\naligned test cases. These models offer dynamic adaptation, enabling updates to test cases as\nthe underlying requirements evolve.\nIn this project, the Test Case Generation Agent was built using the LLaMA 3.1 8B model,\nwhich was fine-tuned on a custom dataset derived from actual SRDs and test case mappings.\nThe system interprets input requirement segments and generates output in the form of\nstructured test cases formatted for direct integration into QA workflows. This capability\naccelerates the testing process and enhances its alignment with business logic.\n3.4 Frameworks: LangChain and LLaMA 3.1 8B\nModern NLP applications often require more than just a single model prediction. They\ndepend on structured reasoning, task decomposition, and contextual chaining\u2014needs that are\naddressed by frameworks such as LangChain. LangChain is an open-source orchestration\nframework designed for large language models (LLMs). It enables chaining multiple steps of\nreasoning and integrates external data sources, APIs, and custom logic within a coherent task\npipeline.\nLangChain plays a vital role in this project by acting as the intermediary that connects user\ninstructions, model prompts, and test case formatting. It allows for prompt engineering\nstrategies that are contextually aware and dynamically adaptable.\nOn the modeling side, LLaMA 3.1 8B represents a leap forward in model size, architecture,\nand accuracy. Designed by Meta AI, LLaMA models are optimized for low-resource\nenvironments while maintaining strong performance in language understanding and"
    },
    {
      "page": 21,
      "text": "12\ngeneration. The 3.1 8B variant, when fine-tuned, becomes capable of producing highly\nstructured and relevant text outputs.\nTogether, these technologies enable a fluid, intelligent system for automated test generation.\nBy leveraging LangChain's orchestration and LLaMA's language modeling, the project\ndelivers outputs that are not only accurate but also production-ready.\n3.5 Summary of Research Gaps and Project Alignment\nAlthough significant progress has been made in both system monitoring and AI-driven\nautomation, integration challenges remain. Many existing dashboards are limited by rigid\narchitectures that do not support real-time updates or automated feedback loops. Similarly,\nwhile large language models have been used in areas like summarization or chatbot\ndevelopment, their potential in QA automation is largely untapped.\nThis project contributes a novel integration of these domains. It combines the monitoring\npower of real-time analytics with the adaptability of AI-generated test scenarios, forming a\nunified QA solution. The literature reviewed supports the feasibility and importance of this\ndirection, while also highlighting the innovation this project introduces.\nBy addressing gaps in real-time observability, test generation efficiency, and model-based\ncontextual reasoning, the project provides a scalable blueprint for future QA systems driven\nby intelligent automation.\nIn essence, this chapter affirms that the proposed solution is not only technically viable but\nalso timely and relevant to the current needs of the software industry. It sets the stage for a\ntransformation in how quality assurance is approached in the age of AI and continuous\ndelivery."
    },
    {
      "page": 22,
      "text": "13\nChapter 4\nTechnical Challenges and Solutions\n4.1 Challenge: Long and Complex SRDs\nProblem:\nSoftware Requirement Documents (SRDs) often span dozens of pages and are authored by\ndifferent stakeholders, leading to a wide variation in formatting, structure, and language.\nSome SRDs follow a strict template with well-defined sections and bullet points, while others\nare more freeform, mixing functional and non-functional requirements without clear\nboundaries. This lack of consistency created a major hurdle in preparing the documents as\nclean, structured input for training the LLaMA 3.1 model. Long paragraphs, nested sub-\npoints, and inconsistent headings made it difficult to extract meaningful and contextually\nbounded requirement snippets.\nSolution:\nTo address this, a custom text chunking mechanism was developed. Instead of relying on\nfixed-size token windows, the system uses semantic chunking \u2014 splitting content based on\nlogical sections such as numbered headings, stepwise procedures, acceptance criteria markers,\nand domain-specific keywords like \u201cThe system shall...\u201d or \u201cExpected Result.\u201d Regular\nexpressions and rule-based parsing were used to break long documents into independently\nmeaningful units while preserving contextual coherence. This preprocessing step significantly\nimproved both the fine-tuning phase and inference-time generation, allowing the model to\nbetter understand the intent of each segment and generate aligned test cases.\n4.2 Challenge: Prompt Injection and Model Hallucination\nProblem:\nWhile using LLaMA 3.1 for generating test cases, an issue was observed where the model\nwould sometimes output unrelated or overly generic test scenarios. These hallucinations\nstemmed from unclear prompts, overlapping instruction semantics, or irrelevant context\ncarried forward from previous generations. In some instances, subtle prompt injection\noccurred when SRDs contained ambiguous or contradictory language that the model\nmisinterpreted as meta-instructions rather than content."
    },
    {
      "page": 23,
      "text": "14\nSolution:\nTo mitigate this, the prompting pipeline was redesigned to include structured templates that\nexplicitly define the expected output format. Each prompt follows a strict instruction-input-\noutput format, using tags such as Requirement:, Generate:, and Note: to isolate and define\neach component. Additionally, a few-shot prompting approach was adopted \u2014 inserting 2\u20133\nhand-crafted examples before the actual SRD snippet \u2014 which significantly anchored the\nmodel\u2019s response behavior. To further reduce hallucinations, a post-processing validation\nscript was implemented. This script checks for irrelevant keywords, excessive vagueness, and\nmismatch between test case conditions and the original requirement. These safeguards\ncollectively improved the model\u2019s reliability and alignment with intended outputs.\n4.3 Challenge: Dashboard Performance Lag Under High-Frequency Input\nProblem:\nThe real-time log analytics dashboard, built using Streamlit, initially exhibited sluggish\nperformance when handling a continuous inflow of log data from parallel automated test runs.\nUsers experienced latency in chart updates, delayed filter responses, and inconsistent\nrendering of visualizations. This issue became more noticeable when dealing with long-\nduration test sessions or high-velocity log streams generated from concurrent environments.\nSolution:\nSeveral optimization strategies were applied to enhance performance. First, log input\nsampling was introduced to process only a representative subset of logs during high-traffic\nintervals. Secondly, caching mechanisms using Streamlit\u2019s native @st.cache_data functions\nwere integrated to avoid redundant recomputation of visual summaries. Additionally, the UI\nupdate frequency was throttled to balance responsiveness with computational load \u2014 limiting\nfull visual refreshes to once every 1\u20132 seconds during peak activity. These enhancements\nensured that the dashboard retained its real-time capabilities without overwhelming the\nfrontend rendering engine.\n4.4 Challenge: Format Preservation in Exported Test Case Documents\nProblem:\nDuring initial deployment of the Test Case Generation Agent, exported .docx files often\ndisplayed\nformatting\ninconsistencies.\nIssues\nincluded\nimproper\nindentation,\nbroken"
    },
    {
      "page": 24,
      "text": "15\nnumbering of test steps, missing section headers, and inconsistent font sizes. This reduced the\nprofessional appearance of the test cases and made them harder for QA teams to interpret or\nimport into TestRail or Jira.\nSolution:\nTo ensure well-structured and visually consistent output, the system transitioned from basic\nstring-to-text export to a more advanced templating method using the python-docx library. A\npredefined document template was created with styles for titles, bullet points, and enumerated\ntest steps. The agent was configured to inject content into this template using programmatic\nsectioning \u2014 each test case was added as a new paragraph block with appropriate styling,\nindentation, and hierarchical numbering. The result was a polished and structured Word\ndocument that could be used directly in real-world QA workflows without further editing."
    },
    {
      "page": 25,
      "text": "16\nChapter 5\nMethodology\nThis chapter describes the methodologies adopted for the development of two primary\ncomponents: the Real-Time Log Analytics Dashboard and the Test Case Generation Agent.\nEach methodology has been systematically designed to address specific challenges and\noptimize software QA processes.\n5.1. Methodology for Real-Time Log Analytics Dashboard\n5.1.1 Data Collection and Aggregation\nInitially, the project focuses on collecting log data from diverse and distributed sources such\nas application servers, network devices, databases, and cloud infrastructures. Given the\nheterogeneous nature of logs (varying formats like text logs, JSON, CSV, XML, and\nstructured logs), it is essential to standardize and aggregate them efficiently.\n5.1.2 Data Processing and Parsing\nOnce the logs are aggregated, they undergo parsing and processing to extract useful fields.\nParsing involves converting unstructured log data into structured and analyzable formats,\nenabling efficient indexing, searching, and analysis.\n5.1.3 Real-Time Analysis and Monitoring\nReal-time analysis involves setting up automated mechanisms to identify and classify\nanomalies, performance bottlenecks, and security threats as they occur. Automated alerts\nbased on predefined thresholds and machine learning-based anomaly detection are\nestablished at this stage.\n5.1.4 Dashboard Design and Visualization\nThe dashboard design prioritizes user-centric visualizations to enhance user understanding\nand response speed. It provides a clear, concise, and intuitive interface enabling stakeholders\nto easily interpret log data and swiftly react to issues."
    },
    {
      "page": 26,
      "text": "17\nFigure 4.1 Flowchart: Log-Analytic Dashboard\n5.2 Methodology for Development of Test Case Generation Agent\nThis section outlines the approach taken in the actual development and validation of the AI-\npowered Test Case Generation Agent. The system has been successfully built and is now\nundergoing rigorous testing across a range of Software Requirement Documents (SRDs). The\nmethodology encompasses requirement analysis, model fine-tuning, scenario generation, and\nintegration into existing QA workflows.\n5.2.1 Requirement Analysis and Contextual Understanding\nThe initial phase focused on a deep contextual understanding of the software requirements.\nThis involved parsing SRDs, identifying domain-specific vocabulary, and pinpointing critical\nand edge-case testing scenarios. Establishing a clear semantic foundation was essential for\nthe language model to generate relevant and actionable test cases.\nTools and Techniques:"
    },
    {
      "page": 27,
      "text": "18\n\uf06c\nRequirement Documentation: Analysis of actual SRDs and user stories used in real-\nworld QA workflows.\n\uf06c\nDomain-Specific Knowledge Extraction: Techniques and rule-based scripts were used\nto identify functional priorities and test boundaries across different domains.\n5.2.2 Data Preparation and Model Fine-Tuning\nA key aspect of this project was the creation of a high-quality, domain-specific dataset to\nfine-tune the LLaMA 3.1 8B language model. The dataset served as the foundation for\ntraining the Test Case Generation Agent to generate accurate, context-aware test scenarios\nbased on varied software requirement documents (SRDs). Given the lack of publicly\navailable datasets that map SRDs to structured test cases, a custom dataset was created in-\nhouse.\n5.2.2.1 SRD to Test Case Mapping\nThe initial step involved manual and semi-automated mapping of SRD content to\ncorresponding test cases. Multiple SRDs were collected from previous QA projects. Each\ndocument was carefully analyzed to identify individual functional requirements, edge\nconditions, and acceptance criteria. These were then mapped to real test cases either written\nmanually by QA professionals or extracted from existing repositories.\nEach test case was paired with its originating requirement to maintain semantic and\ncontextual linkage. This ensured that the model would learn the logical relationship between\nrequirements and the tests designed to validate them.\nTools and Techniques:\n\uf06c\nManual curation and expert QA input\n\uf06c\nRegex-based parsing scripts for requirement segmentation\n5.2.2.2 Conversion to Alpaca Format\nOnce the mappings were finalized, the next phase involved transforming the dataset into\nAlpaca format\u2014a widely used format for fine-tuning instruction-following language models.\nThe format typically includes fields such as:"
    },
    {
      "page": 28,
      "text": "19\ninstruction: Describes the user request (e.g., \u201cGenerate test cases for the following\nSRD segment.\u201d)\ninput: The corresponding SRD snippet or requirement\noutput: The expected test cases (structured in clear, enumerated format)\nThis structure allowed the LLaMA 3.1 8B model to understand the relationship between user\ninstructions and expected responses, making the model more efficient at generalizing new\nSRDs and generating relevant test cases during inference.\nTools and Techniques:\n\uf06c\nPython scripts for JSON conversion\n\uf06c\nValidation and format checking for Alpaca compatibility\n\uf06c\nUse of templates to standardize formatting across all data points\n5.2.2.3 Dataset Evaluation and Refinement\nThe dataset underwent multiple rounds of review to ensure quality and consistency. Incorrect\nmappings, ambiguous inputs, and unclear outputs were flagged and corrected. This iterative\nrefinement process improved model training stability and output reliability.\nAs a result, the final dataset not only reflected the structure of real-world QA tasks but also\noptimized the LLaMA model\u2019s ability to understand and respond to various requirement\ninputs with contextually appropriate test scenarios.\nThis custom dataset played a pivotal role in training an effective Test Case Generation Agent\nand serves as a valuable asset for future research and improvement of AI-assisted QA\nworkflows.\n5.2.4 Integration into QA Workflow\nFollowing development, the agent was integrated with Logic Fruit Technologies' QA\nframework. The generated test cases are validated against different SRDs and then plugged\ninto automated execution pipelines, ensuring seamless transition from generation to execution."
    },
    {
      "page": 29,
      "text": "20\nTools and Techniques:\n\uf06c\nAutomation Framework Integration: Test cases are aligned with PyTest and other in-\nhouse frameworks for automated execution.\n\uf06c\nCI/CD Pipelines: Future integration with tools like Jenkins or GitLab CI is planned to\nenable fully automated QA workflows, including test generation, execution, and\nreporting.\nThis structured methodology ensures that the Test Case Generation Agent not only meets\ntechnical specifications but is also production-ready and adaptable to evolving QA needs.\nFigure 4.2 Flowchart: Test Case Generation Agent"
    },
    {
      "page": 30,
      "text": "21\nChapter 6\nConclusion and Future Work\nAs the Real-Time Log Analytics Dashboard and AI-powered Test Case Generation Agent\nnear full implementation, this chapter revisits the overall objectives, assesses progress made\nso far, and outlines the roadmap for completing and evolving the project. Recent\ndevelopments include the completion and successful integration of the dashboard with the\ncompany\u2019s generic test automation framework and the development of the test case\ngeneration agent, which is currently undergoing rigorous validation using multiple SRDs.\n6.1 Conclusions\nThe project titled \u201cEnhancing Quality Assurance Through Real-Time Log Analytics\nDashboard & Test Case Generation Agent\u201d marks a transformative effort in automating and\nenhancing software quality assurance workflows. As software ecosystems grow more\ndynamic and complex, traditional QA methods are increasingly strained. This project\naddresses those limitations through a forward-looking, intelligent automation strategy.\nThe Real-Time Log Analytics Dashboard has been fully developed, tested, and deployed. It is\nnow integrated with the organization\u2019s generic test automation framework, enabling\ncontinuous and real-time monitoring of log data generated during testing processes. The\ndashboard empowers QA teams with faster diagnostics, better visibility into test execution,\nand collaborative insights across development and testing units.\nParallelly, the Test Case Generation Agent has been successfully built using LangChain and\nthe fine-tuned LLaMA 3.1 8B model. This agent automates the conversion of SRDs into\nstructured test cases formatted as Word documents. The system is now undergoing validation\nthrough tests on diverse SRDs to ensure the accuracy, completeness, and contextual relevance\nof its output.\nTogether, these innovations bridge major gaps in current QA workflows, offering automation,\nintelligence, and real-time observability. They lay the groundwork for more advanced\ncapabilities such as predictive failure analysis, automated bug classification, and autonomous\ntest optimization. Beyond academic and organizational benchmarks, the project stands as a\nscalable foundation for next-generation QA practices."
    },
    {
      "page": 31,
      "text": "22\n6.2 Future Work\nWith both components nearing operational readiness, the remaining scope focuses on fine-\ntuning and full-scale deployment of the Test Case Generation Agent. Ongoing testing with\nvarious SRDs will continue to refine its contextual understanding and scenario generation\nlogic.\nKey areas of upcoming work include optimizing prompt engineering strategies for\nLangChain, enriching the training dataset with more domain-specific requirements, and\nimproving output formatting and relevance. Feedback from QA professionals will be used to\niteratively enhance the agent\u2019s reliability and usability.\nFurther integration with CI/CD pipelines is planned to support automated test generation and\nexecution within continuous delivery environments. As part of long-term improvements, the\nproject may explore adaptive learning techniques, self-healing test scripts, and expanded\nsupport for multilingual and cross-platform testing scenarios."
    },
    {
      "page": 32,
      "text": "23\nReferences\n[1] Kulkarni, A. (2024). Revolutionizing Quality Assurance with Automated Test Case\nGeneration. Retrieved from LinkedIn\n[2] HeadSpin. (2024). How AI Automation is Revolutionizing QA Testing. Retrieved from\nHeadSpin Blog\n[3] TechWell. (2024). The AI Revolution in Software Quality Assurance: A New Era of\nQuality Engineering and Testing. Retrieved from TechWell Insights\n[4] Radha. (2024). AI is Transforming Software Testing: A New Era of Quality Assurance.\nRetrieved from Dev.to\n[5] Devlane. (2024). Test Automation Evolution: Trends and Innovations in QA Engineering.\nRetrieved from Devlane Blog\n[6] DigitalOcean. (2024). AI Testing Tools for Modern QA Teams. Retrieved from\nDigitalOcean Resources\n[7] BrowserStack. (2024). Quality Assurance Tools and Best Practices. Retrieved from\nBrowserStack Guide\n[8] QATestLab. (2024). Test Automation Insights and Trends. Retrieved from QATestLab\nBlog\n[9] Liu, Y., Zhu, Y., & Cui, Y. (2019). Challenges and opportunities towards fast-charging\nbattery materials. Nature Energy, 4, 540\u2013550.\n[10] Vu, V.-B., Tran, D.-H., & Choi, W. (2018). Implementation of constant current and\nconstant voltage charge of inductive power transfer systems. IEEE Transactions on Power\nElectronics, 33(9), 7398\u20137410.\n[11] Few, S. (2013). Information Dashboard Design: Displaying Data for At-a-Glance\nMonitoring (2nd ed.). Analytics Press.\n[12] Knaflic, C. N. (2015). Storytelling with Data: A Data Visualization Guide for Business\nProfessionals. Wiley.\n[13] Anand, S., Burke, E. K., Chen, T. Y., Clark, J., Cohen, M. B., Grieskamp, W., ... & Zhu,\nH. (2013). An orchestrated survey of methodologies for automated software test case\ngeneration. Journal of Systems and Software, 86(8), 1978\u20132001.\n[14] Sanguesa, J. A., Torres-Sanz, V., Garrido, P., Martinez, F. J., & Marquez-Barja, J. M.\n(2021). A Review on Electric Vehicles: Technologies and Challenges. Smart Cities, 4(1),\n372\u2013404."
    },
    {
      "page": 33,
      "text": "24\n[15] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... &\nAmodei, D. (2020). Language Models are Few-Shot Learners. Advances in Neural\nInformation Processing Systems, 33, 1877\u20131901."
    }
  ]
}